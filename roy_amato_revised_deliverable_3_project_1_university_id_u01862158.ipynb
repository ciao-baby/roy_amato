{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNACgGrd8Yabp8NJk2U+0z3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciao-baby/roy_amato/blob/main/roy_amato_revised_deliverable_3_project_1_university_id_u01862158.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cleaned, production-ready credibility Python module (sync, robust, testable).\n",
        "\n",
        "A hybrid NN architecture (Transformer text encoder + metadata MLP) with PyTorch training skeleton and defense of assumptions (novelty).\n",
        "\n",
        "Example code to package & push the model artifact to Hugging Face Hub.\n",
        "\n",
        "A FastAPI integration endpoint suitable for a chatbot (scalable, simple).\n",
        "\n",
        "Unit test examples and basic monitoring/CI suggestions."
      ],
      "metadata": {
        "id": "E5bVkO-mpKdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# credibility.py\n",
        "# ============================================\n",
        "# Production-ready URL credibility scoring module\n",
        "# - Robust HTTP handling\n",
        "# - Extracts metadata, content features and domain signals\n",
        "# - Returns structured JSON suitable for a chatbot or API response\n",
        "# ============================================\n",
        "\n",
        "import time\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from bs4 import BeautifulSoup\n",
        "import tldextract\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "\n",
        "# --- HTTP session with retries ---\n",
        "# Create a single Requests Session configured with retries and timeouts.\n",
        "# Using session + Retry reduces transient network failure impact and improves reliability.\n",
        "_session = requests.Session()\n",
        "_retries = Retry(total=2, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n",
        "_session.mount(\"https://\", HTTPAdapter(max_retries=_retries))\n",
        "_session.mount(\"http://\", HTTPAdapter(max_retries=_retries))\n",
        "\n",
        "def _safe_get(url, timeout=6):\n",
        "    \"\"\"Fetch URL content safely with retries and return text and status code.\n",
        "    - returns (text, status_code)\n",
        "    - raises exception on fatal errors to be handled by callers\n",
        "    \"\"\"\n",
        "    resp = _session.get(url, timeout=timeout, headers={\"User-Agent\": \"CredBot/1.0\"})\n",
        "    resp.encoding = resp.apparent_encoding or resp.encoding\n",
        "    return resp.text, resp.status_code\n",
        "\n",
        "# --- Helper: canonicalize domain and scheme ---\n",
        "# Extracts domain info and canonical host; used for domain-based heuristics and caching keys.\n",
        "def _domain_info(url):\n",
        "    parsed = tldextract.extract(url)\n",
        "    domain = f\"{parsed.domain}.{parsed.suffix}\" if parsed.suffix else parsed.domain\n",
        "    scheme = urlparse(url).scheme or \"http\"\n",
        "    return domain.lower(), scheme.lower()\n",
        "\n",
        "# --- Feature extraction from HTML/text ---\n",
        "# Extract title, visible text, word counts, meta tags, and presence of login/news/ad patterns.\n",
        "def _extract_features(html_text):\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    title = (soup.title.string or \"\").strip() if soup.title else \"\"\n",
        "    # visible text with separators, used for word-count and content-signal heuristics\n",
        "    text = soup.get_text(separator=\" \", strip=True)\n",
        "    word_count = len(text.split())\n",
        "    # meta tags helpful for credibility (author, published-time, canonical)\n",
        "    meta = {}\n",
        "    for name in (\"author\", \"publisher\", \"article:published_time\", \"og:title\", \"og:site_name\"):\n",
        "        tag = soup.find(\"meta\", attrs={\"name\": name}) or soup.find(\"meta\", attrs={\"property\": name})\n",
        "        if tag and tag.get(\"content\"):\n",
        "            meta[name] = tag[\"content\"]\n",
        "    # simple signals\n",
        "    has_author = bool(meta.get(\"author\"))\n",
        "    has_pubtime = bool(meta.get(\"article:published_time\"))\n",
        "    contains_paywall = bool(soup.select_one(\"div.paywall\") or \"subscribe\" in text.lower() and \"read more\" in text.lower())\n",
        "    return {\n",
        "        \"title\": title[:300],\n",
        "        \"text\": text,\n",
        "        \"word_count\": word_count,\n",
        "        \"meta\": meta,\n",
        "        \"has_author\": has_author,\n",
        "        \"has_pubtime\": has_pubtime,\n",
        "        \"contains_paywall\": contains_paywall\n",
        "    }\n",
        "\n",
        "# --- Core heuristic scorer ---\n",
        "# Small, interpretable scoring function that uses domain, scheme and content heuristics.\n",
        "def heuristic_score(domain, scheme, features):\n",
        "    \"\"\"\n",
        "    Start from base and apply heuristic boosts/penalties:\n",
        "    - HTTPS gives +10\n",
        "    - .edu/.gov +20, established .org/.com +5\n",
        "    - low word_count penalty, presence of author/published time bonus\n",
        "    - suspicious TLDs penalized\n",
        "    \"\"\"\n",
        "    score = 50\n",
        "    if scheme == \"https\":\n",
        "        score += 10\n",
        "\n",
        "    if domain.endswith((\".edu\", \".gov\")):\n",
        "        score += 20\n",
        "    elif domain.endswith((\".org\", \".com\")):\n",
        "        score += 5\n",
        "    elif domain.endswith((\".biz\", \".info\", \".xyz\", \".top\", \".pw\")):\n",
        "        score -= 15\n",
        "\n",
        "    wc = features[\"word_count\"]\n",
        "    if wc < 200:\n",
        "        score -= 12\n",
        "    elif wc > 1000:\n",
        "        score += 6\n",
        "\n",
        "    # Author and published time as trust signals\n",
        "    if features[\"has_author\"]:\n",
        "        score += 5\n",
        "    if features[\"has_pubtime\"]:\n",
        "        score += 7\n",
        "    if features[\"contains_paywall\"]:\n",
        "        score += 2  # paywalls often imply publisher business model (weak signal)\n",
        "\n",
        "    # clamp\n",
        "    score = max(0, min(100, score))\n",
        "    return score\n",
        "\n",
        "def score_url(url, fetch_timeout=6):\n",
        "    \"\"\"Top-level function returning structured result for a single URL.\n",
        "    - returns dict with metadata, numeric score and label (\"Low\"/\"Medium\"/\"High\")\n",
        "    - resilient: catches and reports exceptions without raising\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    try:\n",
        "        html, status = _safe_get(url, timeout=fetch_timeout)\n",
        "        domain, scheme = _domain_info(url)\n",
        "        features = _extract_features(html)\n",
        "        score = heuristic_score(domain, scheme, features)\n",
        "        label = \"Low\" if score < 40 else (\"Medium\" if score < 70 else \"High\")\n",
        "        uid = hashlib.sha256(url.encode(\"utf-8\")).hexdigest()[:12]\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"uid\": uid,\n",
        "            \"domain\": domain,\n",
        "            \"scheme\": scheme,\n",
        "            \"status_code\": status,\n",
        "            \"title\": features[\"title\"],\n",
        "            \"word_count\": features[\"word_count\"],\n",
        "            \"has_author\": features[\"has_author\"],\n",
        "            \"has_pubtime\": features[\"has_pubtime\"],\n",
        "            \"score\": score,\n",
        "            \"label\": label,\n",
        "            \"latency_ms\": int((time.time() - start) * 1000)\n",
        "        }\n",
        "    except Exception as exc:\n",
        "        return {\n",
        "            \"url\": url,\n",
        "            \"error\": str(exc),\n",
        "            \"score\": 0,\n",
        "            \"label\": \"Unavailable\",\n",
        "            \"latency_ms\": int((time.time() - start) * 1000)\n",
        "        }\n",
        "\n",
        "# --- Batch helper for multiple URLs (synchronous) ---\n",
        "# Simple convenience wrapper that processes list of URLs and returns results list.\n",
        "def score_urls(urls, timeout=6):\n",
        "    results = []\n",
        "    for url in urls:\n",
        "        results.append(score_url(url, fetch_timeout=timeout))\n",
        "    return results\n",
        "\n",
        "# If run as script: quick CLI\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ” Credibility scoring (production-ready toy). Enter URLs separated by spaces:\")\n",
        "    urls = input(\"> \").split()\n",
        "    res = score_urls(urls)\n",
        "    for r in res:\n",
        "        if r.get(\"error\"):\n",
        "            print(f\"URL: {r['url']}\\n  âŒ {r['error']}\\n\")\n",
        "        else:\n",
        "            print(f\"URL: {r['url']}\\n  Domain: {r['domain']}\\n  Score: {r['score']} ({r['label']})\\n\")\n"
      ],
      "metadata": {
        "id": "alfLUUAppNA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem: Credibility is multifaceted: content semantics, writing style, topicality (publication date), and domain-level trust.\n",
        "\n",
        "Idea (novelty): Use a hybrid model: a pretrained Transformer (e.g., DistilBERT) encodes the article text into a semantic embedding; metadata features (domain-encoded embeddings, TLD, HTTPS flag, word count, author presence, published time delta, social signals if available) are concatenated and passed through an MLP. The combined network learns late fusion weights to judge credibility.\n",
        "\n",
        "Why novel: The novelty is in explicitly learning attention-like gating between semantic and metadata channels: a small gating MLP outputs per-channel weights that adaptively weight semantic vs. metadata contributions for each sample. This counters the class assumption that \"text-only\" is sufficient â€” the model learns when to trust metadata more (e.g., short content from unknown TLDs) vs. semantic content (opinionated long-form text).\n",
        "\n",
        "Assumptions & counterargument: Classic assumption: BERT-style semantics alone capture credibility. Counter: metadata (domain, author, timestamp) carry orthogonal signals. The gating mechanism lets the model adapt if metadata is noisy (learned weighting may downweight it). This trade-off and gating is the model novelty."
      ],
      "metadata": {
        "id": "eKtXirqtpVzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.py\n",
        "# 3-5 lines of comments per block describing purpose and functionality.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class HybridCredModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid model:\n",
        "    - Text encoder: pretrained Transformer (frozen or fine-tuned)\n",
        "    - Metadata MLP to encode numeric/categorical features\n",
        "    - Gating mechanism to weight channels and final MLP for regression/classification\n",
        "    \"\"\"\n",
        "    def __init__(self, transformer_name=\"distilbert-base-uncased\", meta_dim=16, freeze_transformer=False):\n",
        "        super().__init__()\n",
        "        # Text encoder: pretrained transformer -> pooled vector\n",
        "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
        "        if freeze_transformer:\n",
        "            for p in self.transformer.parameters():\n",
        "                p.requires_grad = False\n",
        "        hidden_size = self.transformer.config.hidden_size\n",
        "\n",
        "        # Metadata MLP: encodes numeric/categorical metadata into meta_dim vector\n",
        "        self.meta_net = nn.Sequential(\n",
        "            nn.Linear(10, 64),  # assume 10 numeric/categorical features preprocessed\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, meta_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Gating network: produces two weights to combine text/meta\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(hidden_size + meta_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Final predictor MLP -> scalar score (regression) or logits\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(hidden_size + meta_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)  # regression score 0-100; use BCE or softmax for classification\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, meta_features):\n",
        "        # Text embedding: CLS/pool\n",
        "        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        # use pooled output (for BERT-like) or mean pool for some models\n",
        "        if hasattr(out, \"pooler_output\") and out.pooler_output is not None:\n",
        "            txt_emb = out.pooler_output\n",
        "        else:\n",
        "            # mean pool last_hidden_state\n",
        "            txt_emb = (out.last_hidden_state * attention_mask.unsqueeze(-1)).sum(1)\n",
        "            txt_emb = txt_emb / attention_mask.sum(1, keepdim=True)\n",
        "\n",
        "        meta_emb = self.meta_net(meta_features)\n",
        "        combined = torch.cat([txt_emb, meta_emb], dim=1)\n",
        "        gates = self.gate(combined)  # [batch, 2]\n",
        "        # weighted combination (not strictly necessary but demonstrates gating)\n",
        "        gated = gates[:, 0:1] * txt_emb + gates[:, 1:2] * meta_emb\n",
        "        final_in = torch.cat([txt_emb, meta_emb], dim=1)\n",
        "        out_score = self.pred(final_in).squeeze(-1)\n",
        "        return out_score, gates\n"
      ],
      "metadata": {
        "id": "-uxnLP3Ipmvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use MSE for regression to a 0â€“100 score or cross-entropy if mapping to classes.\n",
        "\n",
        "Preprocess metadata so all features are numeric and scaled (one-hot for categorical TLD or domain clusters).\n",
        "\n",
        "If labeled dataset is small: freeze transformer and train classifier + meta MLP; then optionally unfreeze last transformer layers.\n",
        "\n",
        "Use mix of loss terms (primary MSE + optional regularization that penalizes too-large gate entropy to prevent collapse)."
      ],
      "metadata": {
        "id": "e7nQLaY2qEDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py (skeleton)\n",
        "# - Load dataset of {url,text,meta_features,score}\n",
        "# - Tokenize text with a transformer tokenizer and train HybridCredModel\n",
        "import torch, random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from model import HybridCredModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "class CredDataset(Dataset):\n",
        "    def __init__(self, records):\n",
        "        self.records = records\n",
        "    def __len__(self): return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        rec = self.records[idx]\n",
        "        toks = tokenizer(rec[\"text\"][:2000], truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "        meta = torch.tensor(rec[\"meta_features\"], dtype=torch.float32)\n",
        "        score = torch.tensor(rec[\"score\"], dtype=torch.float32)\n",
        "        return toks[\"input_ids\"].squeeze(0), toks[\"attention_mask\"].squeeze(0), meta, score\n",
        "\n",
        "# Example: prepare records list with 'text','meta_features'(len=10),'score'\n",
        "train_records = []  # populate with real data\n",
        "train_ds = CredDataset(train_records)\n",
        "loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = HybridCredModel(transformer_name=\"distilbert-base-uncased\", meta_dim=16).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(4):\n",
        "    model.train()\n",
        "    for input_ids, attention_mask, meta, score in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        meta = meta.to(device)\n",
        "        score = score.to(device)\n",
        "        pred, gates = model(input_ids, attention_mask, meta)\n",
        "        loss = criterion(pred, score)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    print(f\"Epoch {epoch} done, loss={loss.item():.4f}\")\n",
        "# After training, save model and tokenizer\n",
        "torch.save(model.state_dict(), \"hybrid_cred_model.pt\")\n",
        "tokenizer.save_pretrained(\"tokenizer/\")\n"
      ],
      "metadata": {
        "id": "edts2kIaqb2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DyyQvYluq2ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy_to_hf.py\n",
        "# Upload model files (weights + tokenizer + small wrapper) to HF Hub\n",
        "from huggingface_hub import HfApi, Repository, upload_file\n",
        "import os\n",
        "\n",
        "HF_USER = \"your-hf-username\"\n",
        "REPO_NAME = \"hybrid-credibility-model\"\n",
        "LOCAL_FOLDER = \"./hf_model\"\n",
        "\n",
        "# ensure files exist: hybrid_cred_model.pt, tokenizer/* etc.\n",
        "os.makedirs(LOCAL_FOLDER, exist_ok=True)\n",
        "# copy files into LOCAL_FOLDER (implementation detail omitted)\n",
        "\n",
        "api = HfApi()\n",
        "# create repo if not exists (requires HF token via env HF_TOKEN)\n",
        "api.create_repo(repo_id=f\"{HF_USER}/{REPO_NAME}\", private=False, exist_ok=True)\n",
        "\n",
        "# upload files\n",
        "upload_file(path_or_fileobj=\"hybrid_cred_model.pt\", path_in_repo=\"hybrid_cred_model.pt\", repo_id=f\"{HF_USER}/{REPO_NAME}\")\n",
        "for fname in [\"tokenizer/vocab.json\", \"tokenizer/tokenizer.json\", \"tokenizer/special_tokens_map.json\"]:\n",
        "    if os.path.exists(fname):\n",
        "        upload_file(path_or_fileobj=fname, path_in_repo=fname, repo_id=f\"{HF_USER}/{REPO_NAME}\")\n",
        "\n",
        "print(\"Uploaded artifacts to Hugging Face Hub.\")\n"
      ],
      "metadata": {
        "id": "LoJQUcKFrAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For full HF Model Card, include modelcard.md describing intended use, limitations, and evaluation metrics.\n",
        "\n",
        "Use transformers AutoModelForSequenceClassification if you want Hugging Face-native model class; for custom hybrid, provide a wrapper class in the repo that reconstructs the PyTorch model."
      ],
      "metadata": {
        "id": "iiCteSxVrFze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "# 3-5 lines of comments for blocks: FastAPI app to serve scoring endpoint to chatbot\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, HttpUrl\n",
        "from credibility import score_url, score_urls\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI(title=\"Credibility API\")\n",
        "\n",
        "class SingleURL(BaseModel):\n",
        "    url: HttpUrl\n",
        "\n",
        "class BatchURLs(BaseModel):\n",
        "    urls: list[HttpUrl]\n",
        "\n",
        "@app.post(\"/score\")\n",
        "async def score(single: SingleURL):\n",
        "    # Single synchronous scoring (fast); suitable for Chatbot integration\n",
        "    result = score_url(single.url)\n",
        "    if result.get(\"error\"):\n",
        "        raise HTTPException(status_code=502, detail=result[\"error\"])\n",
        "    return result\n",
        "\n",
        "@app.post(\"/score_batch\")\n",
        "async def score_batch(batch: BatchURLs):\n",
        "    results = score_urls(batch.urls)\n",
        "    return {\"results\": results}\n",
        "\n",
        "# Run with: uvicorn app:app --host 0.0.0.0 --port 8000\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, workers=2)\n"
      ],
      "metadata": {
        "id": "Im8pV38vrMgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For high concurrency in production, run with multiple workers behind a load balancer. Use an async HTTP client (httpx.AsyncClient) to parallelize fetches if you expect many concurrent requests â€” the current module is synchronous to keep behavior simpler for safe resource management."
      ],
      "metadata": {
        "id": "c3h14br6rRiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tests/test_credibility.py\n",
        "# Basic unit tests for the scoring module (use pytest)\n",
        "import pytest\n",
        "from credibility import score_url\n",
        "\n",
        "def test_invalid_url():\n",
        "    res = score_url(\"http://nonexistent.invalid.example\")\n",
        "    assert res[\"score\"] == 0 or res[\"label\"] == \"Unavailable\"\n",
        "\n",
        "def test_google_com():\n",
        "    res = score_url(\"https://www.google.com\")\n",
        "    assert \"domain\" in res\n",
        "    assert 0 <= res.get(\"score\", 0) <= 100\n",
        "\n",
        "# Add more tests for local HTML fixtures to deterministically test heuristics\n"
      ],
      "metadata": {
        "id": "P3PaiterrZjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I\n",
        "\n",
        "Unit tests: cover input parsing, network failure handling, domain extraction, small HTML fixtures.\n",
        "\n",
        "Integration tests: run FastAPI endpoints using TestClient and mocked HTTP responses (e.g., responses or httpretty) to avoid network dependency.\n",
        "\n",
        "Model evaluation: use MSE / MAE for regression or accuracy/F1 for classes. Report ROC AUC if probabilistic. Provide a held-out test set and cross-validation.\n",
        "\n",
        "Stress testing & performance: run load tests (e.g., locust or k6) against FastAPI, measure p95 latency. Use caching (Redis) for repeated URLs to avoid re-fetch.\n",
        "\n",
        "II\n",
        "\n",
        "Metrics: request latency, request success/fail rates, p95 latency, model inference time, cache hit ratio. Export to Prometheus.\n",
        "\n",
        "Logging: structured logs (JSON) including URL, uid, status_code, score, latency, and errors. Mask PII.\n",
        "\n",
        "Alerts: high error rate or sudden drop in average score might indicate crawling problems or dataset drift.\n",
        "\n",
        "III\n",
        "\n",
        "Validate/whitelist input domains or use a safe browsing list if your service will fetch arbitrary URLs.\n",
        "\n",
        "Run fetches from an isolated environment (container) with network egress control.\n",
        "\n",
        "Respect robots.txt if needed, and rate-limit requests per origin.\n",
        "\n",
        "Limit HTML parse size and timeouts to avoid DoS."
      ],
      "metadata": {
        "id": "-X1tQS8_rkUM"
      }
    }
  ]
}